{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jashkine/RAG_Pipeline/blob/main/Workbook_Exercises_Module_3_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction to Multi-Query RAG\n",
        "\n",
        "![](https://i.ibb.co/26x6qpD/Screenshot-2024-11-02-at-19-34-48.png)\n",
        "\n",
        "### What is Multi-Query RAG?\n",
        "\n",
        "In traditional RAG systems, a single query is used to retrieve relevant documents to feed into the language model. **Multi-Query RAG** improves upon this by:\n",
        "- Generating multiple queries from a single user input.\n",
        "- Retrieving documents for each query, which are then aggregated.\n",
        "- This increases the diversity of the retrieved documents, leading to richer and more accurate responses from the model.\n",
        "\n",
        "### Benefits of Multi-Query RAG:\n",
        "1. **More Contextual Information**: By retrieving documents from multiple perspectives, the model gets access to more varied information.\n",
        "2. **Improved Response Quality**: The model has more data to generate a better response, reducing the risk of hallucinations.\n",
        "\n",
        "### How does Multi-Query RAG work?\n",
        "\n",
        "1. **Initial Query Generation**: A single user question is split into multiple related queries.\n",
        "2. **Parallel Retrieval**: Each query retrieves relevant documents or passages from a knowledge base.\n",
        "3. **Fusion of Information**: The retrieved documents are combined and sent to the model to generate a response.\n",
        "\n",
        "In the next part, we will implement this architecture step by step.\n"
      ],
      "metadata": {
        "id": "ay2LV1INe52B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Multi-Query RAG Implementation (Part 1)\n",
        "\n",
        "In this section, we will implement the first part of the Multi-Query RAG system.\n",
        "\n",
        "### Key Steps:\n",
        "1. **Query Decomposition**: Split a single user query into multiple queries.\n",
        "2. **Retrieve Documents for Each Query**: Use embeddings and a vector database to retrieve relevant documents for each query.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FnD3nUFLfNnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install pinecone"
      ],
      "metadata": {
        "id": "Dz2CRAd5lKTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pinecone\n",
        "import json"
      ],
      "metadata": {
        "id": "SioOrqRGlMBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(api_key=\"YOUR_KEY\")\n",
        "pinecone_clinet = Pinecone(api_key=\"YOUR_API_KEY\")\n",
        "index_name = \"faq-embeddings\"\n",
        "# Connecting to an Index\n",
        "index = pinecone_clinet.Index(index_name)"
      ],
      "metadata": {
        "id": "X1DpaNj8lRwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_model(query, openai_client, model=\"text-embedding-3-small\"):\n",
        "  # Getting the embedding\n",
        "  response = openai_client.embeddings.create(\n",
        "      model=model,\n",
        "      input=query\n",
        "  )\n",
        "\n",
        "  embedding = response.data[0].embedding\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "saEWvy-g5QiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
      ],
      "metadata": {
        "id": "0J6qj6dP5RFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": f\"\"\"\n",
        "                    You are a helpfull E-Commerce assistant helping customers with their general questions regarding policies and procedures when buying in our store.\n",
        "                    Our store sells e-books and courses for IT professionals.\n",
        "                    \"\"\",\n",
        "                }\n",
        "\n",
        "def prompt_builder(system_message, context):\n",
        "  return system_message.format(context)"
      ],
      "metadata": {
        "id": "ABkL0zMG61gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def candidates_generation(query, openai_client, n_candidates=5):\n",
        "\n",
        "  system_prompt = \"\"\n",
        "\n",
        "  messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "\n",
        "  response = openai_client.client.chat.completions.create(\n",
        "      model=\"gpt-4o\",\n",
        "      messages=messages,\n",
        "      max_tokens=1500,\n",
        "      response_format={ \"type\": \"json_object\" }\n",
        "    )\n",
        "\n",
        "  response_content = response.choices[0].message.content\n",
        "  response_type = json.loads(response_content)\n",
        "  return response_type"
      ],
      "metadata": {
        "id": "4B8cvizvqbRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find the most similar FAQ in Pinecone\n",
        "def retrieve_faq(query_embedding, index):\n",
        "    query_result = index.query(\n",
        "        queries=[query_embedding],\n",
        "        top_k=top_k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    return query_result['matches'][0]['metadata']['answer']"
      ],
      "metadata": {
        "id": "mYBc4gUTmG62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_documents(retrieved_docs):\n",
        "    return \"\\n\\n\".join(retrieved_docs)"
      ],
      "metadata": {
        "id": "_ycdKFHP5lc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_query_rag_chatbot(query, openai_client, index):\n",
        "\n",
        "    # Step 1: Get multi-representation\n",
        "    candidates = False\n",
        "\n",
        "    # Step 2: Retrieve the most relevant FAQ from Pinecone (for each candidate)\n",
        "    relevant_docs = []\n",
        "    for key, candidate in candidates.items():\n",
        "      candidate_embedding = False\n",
        "      best_match = False\n",
        "      relevant_docs.append(best_match)\n",
        "\n",
        "    # Step 3: Combine docs\n",
        "    context = False\n",
        "\n",
        "    # Step 4: Augment the query with context\n",
        "    augmented_prompt = prompt_builder(system_prompt, context)\n",
        "\n",
        "    messages = [{\"role\": \"system\",\"content\": augmented_prompt},\n",
        "                {\"role\": \"user\",\"content\": query}]\n",
        "\n",
        "    # Step 4: Use OpenAI to generate a response\n",
        "    response = openai_client.client.chat.completions.create(\n",
        "      model=\"gpt-4o\",\n",
        "      messages=messages,\n",
        "      max_tokens=250\n",
        "    )\n",
        "\n",
        "    return response.choices[0].text"
      ],
      "metadata": {
        "id": "BEkLQcgzmQ-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "while True:\n",
        "  query = input()\n",
        "  response = multi_query_rag_chatbot(query)\n",
        "  print(f\"User: {query}\")\n",
        "  print(f\"Bot: {response}\")"
      ],
      "metadata": {
        "id": "Qa4RFR_Q7NSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 4. What is Fusion RAG?\n",
        "\n",
        "### Introduction to Fusion RAG\n",
        "\n",
        "Fusion RAG improves upon traditional RAG by fusing multiple sources of information, such as different document types or databases. Itâ€™s similar to Multi-Query RAG but instead of using multiple queries, it uses multiple **sources** of data to retrieve relevant information.\n",
        "\n",
        "### How Does Fusion RAG Work?\n",
        "1. **Multi-source Retrieval**: Information is retrieved from multiple sources (databases, websites, document types).\n",
        "2. **Context Fusion**: The retrieved information is combined and fed into the model.\n",
        "3. **Response Generation**: The model generates a response using the fused context.\n",
        "\n",
        "Fusion RAG is especially useful when we want to combine structured and unstructured data or information from different domains.\n"
      ],
      "metadata": {
        "id": "2AaTBwc7fcIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Exercise: Implement Fusion RAG\n",
        "\n",
        "### Task:\n",
        "\n",
        "In this exercise, you will implement a Fusion RAG system that retrieves documents in the same way we used in the multi-query RAG, but now, we rag each doc with reciprotial fusion rank.\n",
        "\n",
        "![](https://i.ibb.co/xs33BNz/Screenshot-2024-11-02-at-19-38-53.png)\n",
        "\n",
        "**Steps:**\n",
        "1. Create multiple candidate (4 candidates) for the original user-query\n",
        "2. Get top 5 documents for each candidate + original query\n",
        "3. Run the reciprotial fusion ranking algorithm to re-rank all results\n",
        "4. Take top 4 documents\n",
        "5. Generate a reuslt based on the newly created context\n",
        "\n"
      ],
      "metadata": {
        "id": "Y9l2OAX9fecu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_faq_top_n(query_embedding, index, top_k=5):\n",
        "    # TODO: Implement Top N Retrieval function"
      ],
      "metadata": {
        "id": "3E72FC7GCi0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://weaviate.io/img/blog/2023-01-03-hybrid-search-explained/RRF-calculation.png)"
      ],
      "metadata": {
        "id": "iD8Z3dB5YvO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reciprocal_rank_fusion(results, k=60, top_n=5):\n",
        "    # TODO: Implement RRF function"
      ],
      "metadata": {
        "id": "rHmy6kYeFir8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fusion_rag_chatbot(query, openai_client, index):\n",
        "\n",
        "    # Step 1: Get multi-representation\n",
        "    candidates = candidates_generation(query, openai_client, n_candidates=4)\n",
        "\n",
        "    # Step 2: Retrieve the most relevant FAQ from Pinecone (for each candidate)\n",
        "    relevant_docs = []\n",
        "    for key, candidate in candidates.items():\n",
        "      candidate_embedding = embedding_model(candidate, openai_client)\n",
        "      best_match = False\n",
        "      relevant_docs.append(best_match)\n",
        "\n",
        "    # Step 3: Ranking\n",
        "    ranked_docs = False\n",
        "\n",
        "    # Step 4: Combine docs\n",
        "    context = combine_documents(ranked_docs)\n",
        "\n",
        "    # Step 5: Augment the query with context\n",
        "    augmented_prompt = prompt_builder(system_prompt, context)\n",
        "\n",
        "    messages = [{\"role\": \"system\",\"content\": augmented_prompt},\n",
        "                {\"role\": \"user\",\"content\": query}]\n",
        "\n",
        "    # Step 6: Use OpenAI to generate a response\n",
        "    response = openai_client.client.chat.completions.create(\n",
        "      model=\"gpt-4o\",\n",
        "      messages=messages,\n",
        "      max_tokens=250\n",
        "    )\n",
        "\n",
        "    return response.choices[0].text"
      ],
      "metadata": {
        "id": "OshhRHBLfdQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. What is HyDE RAG?\n",
        "\n",
        "### What is HyDE RAG?\n",
        "\n",
        "HyDE (Hypothetical Document Embeddings) RAG takes a different approach from traditional RAG. Instead of retrieving real documents, HyDE generates **hypothetical** documents based on the user's query and then retrieves information that aligns with those hypothetical documents.\n",
        "\n",
        "![](https://i.ibb.co/v3grFj3/Screenshot-2024-11-02-at-19-41-36.png)\n",
        "\n",
        "### How Does HyDE RAG Work?\n",
        "1. **Query Understanding**: The user query is transformed into a hypothetical document or context by the model.\n",
        "2. **Document Retrieval**: The system retrieves documents that align with the generated hypothetical document.\n",
        "3. **Response Generation**: The final response is generated based on the retrieved real documents and the hypothetical context.\n",
        "\n",
        "This approach is useful when relevant documents might not exist in the knowledge base, but the hypothetical documents guide the retrieval process to related information.\n"
      ],
      "metadata": {
        "id": "tHEU8Q3Bfp63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. HyDE RAG Implementation (Part 1)\n",
        "\n",
        "We will implement the first part of HyDE RAG, where the model generates hypothetical documents based on the user's query.\n",
        "\n"
      ],
      "metadata": {
        "id": "FzitGiY6fsZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Generate hypothetical document from query\n",
        "def generate_hypothetical_document(query, openai_client):\n",
        "    prompt = \"\"\n",
        "    messages = [{\"role\": \"system\", \"content\": prompt}]\n",
        "\n",
        "    response = openai_client.client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages,\n",
        "        max_tokens=1500,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].text"
      ],
      "metadata": {
        "id": "PbhWyCwjfoeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"How do neural networks learn?\"\n",
        "hypothetical_doc = generate_hypothetical_document(query, openai_client)\n",
        "print(\"Hypothetical Document:\", hypothetical_doc)\n"
      ],
      "metadata": {
        "id": "r8-VqqX7HYLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 10. HyDE RAG Implementation (Part 2)\n",
        "\n",
        "In this part, we will retrieve documents based on the hypothetical context and generate the final response.\n",
        "\n"
      ],
      "metadata": {
        "id": "sQWH01ERfvld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hypo_chatbot(query, openai_client, index):\n",
        "\n",
        "    # Step 1: Get multi-representation\n",
        "    hypo_candidate = False\n",
        "\n",
        "    # Step 2: Retrieve the most relevant FAQ from Pinecone\n",
        "    candidate_embedding = embedding_model(hypo_candidate, openai_client)\n",
        "\n",
        "    # Step 3: get real answer\n",
        "    best_match = retrieve_faq(candidate_embedding, index)\n",
        "\n",
        "    # Step 4: Augment the query with context\n",
        "    augmented_prompt = prompt_builder(system_prompt, context)\n",
        "\n",
        "    messages = [{\"role\": \"system\",\"content\": augmented_prompt},\n",
        "                {\"role\": \"user\",\"content\": query}]\n",
        "\n",
        "    # Step 5: Use OpenAI to generate a response\n",
        "    response = openai_client.client.chat.completions.create(\n",
        "      model=\"gpt-4o\",\n",
        "      messages=messages,\n",
        "      max_tokens=250\n",
        "    )\n",
        "\n",
        "    return response.choices[0].text"
      ],
      "metadata": {
        "id": "yuLvD9Sofya5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "response = hyde_rag_chatbot(\"How do neural networks learn?\")\n",
        "print(\"Generated Response:\", response)"
      ],
      "metadata": {
        "id": "5CHPN5AaICPk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}